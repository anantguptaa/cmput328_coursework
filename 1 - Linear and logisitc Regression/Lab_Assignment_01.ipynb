{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54164037",
      "metadata": {
        "id": "54164037"
      },
      "source": [
        "# CMPUT 328 â€” Logistic Regression Lab Assignment\n",
        "\n",
        "**Total Weight:** 5% of course grade  \n",
        "**Part 1 (Lab):** 2%  \n",
        "**Part 2 (Take-home):** 2%  \n",
        "**Deadline:** Sunday, September 21\n",
        "\n",
        "**Tools:** Google Colab + Gemini (AI assistance allowed, but cite and document).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bce060c",
      "metadata": {
        "id": "4bce060c"
      },
      "source": [
        "## ðŸ”§ Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b4c2ca",
      "metadata": {
        "id": "c4b4c2ca"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch + torchvision (if needed)\n",
        "# !pip install -q torch torchvision\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad70771",
      "metadata": {
        "id": "7ad70771"
      },
      "source": [
        "## ðŸ“¦ Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee628ca",
      "metadata": {
        "id": "fee628ca"
      },
      "outputs": [],
      "source": [
        "# Transform: normalize MNIST images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "])\n",
        "\n",
        "# Download MNIST\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split into train (50k) and validation (10k)\n",
        "train_size = 50_000\n",
        "val_size = len(train_ds) - train_size\n",
        "train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7df6b4",
      "metadata": {
        "id": "9b7df6b4"
      },
      "source": [
        "# Part 1 â€” In-Lab (2%)\n",
        "\n",
        "Implement and train Logistic Regression on MNIST.  \n",
        "Complete the following steps **during the lab (3 hours)**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94437854",
      "metadata": {
        "id": "94437854"
      },
      "source": [
        "### 1. Baseline Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "899c15a2",
      "metadata": {
        "id": "899c15a2"
      },
      "outputs": [],
      "source": [
        "# TODO: Define Logistic Regression model using nn.Linear(784, 10).\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, in_dim=28*28, out_dim=10):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "model = LogisticRegression().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd1f6163",
      "metadata": {
        "id": "fd1f6163"
      },
      "source": [
        "### 2. Training Loop (SGD + CrossEntropyLoss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89331017",
      "metadata": {
        "id": "89331017"
      },
      "outputs": [],
      "source": [
        "# TODO: Write training loop\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    pass  # TODO: implement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c4782a",
      "metadata": {
        "id": "62c4782a"
      },
      "source": [
        "### 3. Regularization (L2, L1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf4346a",
      "metadata": {
        "id": "eaf4346a"
      },
      "outputs": [],
      "source": [
        "# TODO: Re-run training with L2 (via weight_decay).\n",
        "# TODO: Implement manual L1 penalty and compare results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1c4f60",
      "metadata": {
        "id": "9d1c4f60"
      },
      "source": [
        "### 4. Optimizer Swap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100f9ec0",
      "metadata": {
        "id": "100f9ec0"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace SGD with Adam optimizer and compare convergence speed. You can try plotting the loss or accuracy curves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beca4512",
      "metadata": {
        "id": "beca4512"
      },
      "source": [
        "### 5. Visualization of Learned Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db25b31",
      "metadata": {
        "id": "9db25b31"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot each classâ€™s weight vector as 28x28 image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b659b6",
      "metadata": {
        "id": "21b659b6"
      },
      "source": [
        "# Part 2 â€” Take-Home (2%)\n",
        "\n",
        "Extend your experiments and write a short (â‰ˆ1 page) report.  \n",
        "**Deadline:** One week after the lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d4ada9",
      "metadata": {
        "id": "50d4ada9"
      },
      "source": [
        "### 1. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d805afc",
      "metadata": {
        "id": "7d805afc"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute confusion matrix on test set predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19317c4e",
      "metadata": {
        "id": "19317c4e"
      },
      "source": [
        "### 2. Reduced Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa208ac5",
      "metadata": {
        "id": "fa208ac5"
      },
      "outputs": [],
      "source": [
        "# TODO: Train logistic regression with only 10%, 25%, 50% of train data.\n",
        "# Plot validation accuracy vs dataset size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9012d4b9",
      "metadata": {
        "id": "9012d4b9"
      },
      "source": [
        "### 3. Noisy Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4407027",
      "metadata": {
        "id": "c4407027"
      },
      "outputs": [],
      "source": [
        "# TODO: Randomly flip 10% of training labels, retrain, and measure accuracy drop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88b2990",
      "metadata": {
        "id": "c88b2990"
      },
      "source": [
        "### 4. Short Report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56ef0b3",
      "metadata": {
        "id": "f56ef0b3"
      },
      "source": [
        "- Which regularization worked better (L1 vs L2)?  \n",
        "- Which optimizer was more effective (SGD vs Adam)?  \n",
        "- What does the confusion matrix reveal?  \n",
        "- How does performance degrade with reduced or noisy data?  \n",
        "- How did Gemini help (include sample prompts + modifications)?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}