{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5b5fcef",
      "metadata": {
        "id": "b5b5fcef"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/Merged_XOR_Notebook.ipynb)\n",
        "\n",
        "> Click the badge above to open this notebook directly in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e4005c",
      "metadata": {
        "id": "05e4005c"
      },
      "source": [
        "# XOR Learning with Neural Networks\n",
        "\n",
        "In this notebook, we will explore how a neural network can learn the XOR function, a classic problem in the history of artificial intelligence. The XOR (exclusive OR) problem is not linearly separable, which means a simple perceptron cannot solve it. This motivated the development of multi-layer neural networks.\n",
        "\n",
        "We will study two approaches:\n",
        "- **Part 1: Using built-in backpropagation** (leveraging modern libraries for automatic differentiation)\n",
        "- **Part 2: Implementing manual backpropagation** (to understand the math and mechanics behind the learning process)\n",
        "\n",
        "By the end of this notebook, you should gain intuition about how neural networks learn non-linear decision boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e714b0c0",
      "metadata": {
        "id": "e714b0c0"
      },
      "source": [
        "# Part 1: XOR with Built-in Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9dbc6b",
      "metadata": {
        "id": "2d9dbc6b"
      },
      "outputs": [],
      "source": [
        "# This notebook implements a neural net to apprximate XOR function using PyTorch\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4165be54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4165be54",
        "outputId": "e60ee050-1256-4529-8990-cbaa83fd72cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  tensor([[1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.]])\n",
            "output:  tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.tensor([[1.0,0.0,0.0,1.0],[0.0,0.0,1.0,1.0]],dtype=torch.float32) # 2x4 matrix\n",
        "X = torch.transpose(X,0,1)\n",
        "Y = torch.tensor([[1.0,0.0,1.0,0.0]],dtype=torch.float32)                   # 1x4 vector\n",
        "Y = torch.transpose(Y,0,1)\n",
        "print(\"input: \", X)\n",
        "print(\"output: \", Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a99603",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5a99603",
        "outputId": "d989a074-2475-49a9-eecf-d18cd83e7694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.26149123907089233\n",
            "loss: 0.12390542030334473\n",
            "loss: 0.009003173559904099\n",
            "loss: 0.003408163320273161\n",
            "loss: 0.0019757780246436596\n",
            "loss: 0.0013575187185779214\n",
            "loss: 0.0010208551539108157\n",
            "loss: 0.0008116937824524939\n",
            "loss: 0.000670215580612421\n",
            "loss: 0.0005686600343324244\n",
            "tensor([[0.9772],\n",
            "        [0.0169],\n",
            "        [0.9786],\n",
            "        [0.0266]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "# parameters of neural net\n",
        "W1 = Variable(torch.torch.FloatTensor(2, 8).uniform_(-1, 1), requires_grad=True) # 2x8 matrix\n",
        "b1 = Variable(torch.zeros((1,8)), requires_grad=True)                            # 1x8 matrix\n",
        "W2 = Variable(torch.torch.FloatTensor(8, 1).uniform_(-1, 1), requires_grad=True) # 8x1 matrix\n",
        "b2 = Variable(torch.zeros([1]), requires_grad=True)                              # scalar\n",
        "\n",
        "learning_rate = 0.05\n",
        "optimizer = torch.optim.SGD([W1, b1, W2, b2], lr=learning_rate, momentum=0.9)    # Torch optimizer\n",
        "\n",
        "loss_fn = torch.nn.MSELoss() # Eclidean loss function\n",
        "\n",
        "for step in range(10000):\n",
        "\n",
        "  # forward pass\n",
        "  Z1 = torch.mm(X,W1)    # 4x8 matrix\n",
        "  Z2 = Z1 + b1           # 4x8 matrix\n",
        "  Z3 = torch.sigmoid(Z2) # 4x8 matrix\n",
        "  Z4 = torch.mm(Z3,W2)   # 4x1 vector\n",
        "  Z5 = Z4 + b2           # 4x1 vector\n",
        "  Yp = torch.sigmoid(Z5) # 4x1 vector\n",
        "\n",
        "  # backward pass\n",
        "  optimizer.zero_grad()  # zero out previous gradients\n",
        "  loss = loss_fn(Yp,Y)   # compute loss\n",
        "  loss.backward()        # calculate gradients\n",
        "  #Yp.backward(Yp-Y)     # or, apply gradient of loss at Yp!\n",
        "  #Z5.backward(Yp*(1.0-Yp)*(Yp-Y)) # or, apply gradient of Yp at Z5!\n",
        "  optimizer.step()       # apply new gradients\n",
        "\n",
        "  if step%1000 == 0:\n",
        "    print(\"loss:\",loss.item())\n",
        "\n",
        "print(Yp)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89cc939a",
      "metadata": {
        "id": "89cc939a"
      },
      "source": [
        "## Let's do the same using PyTorch's nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf31fa0",
      "metadata": {
        "id": "abf31fa0"
      },
      "outputs": [],
      "source": [
        "# Define a neural net architecture\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 8)\n",
        "        self.fc = nn.Linear(8, 15)\n",
        "        self.fc2 = nn.Linear(15, 1)\n",
        "\n",
        "    # forward pass of the neural net\n",
        "    def forward(self, X):\n",
        "        #return torch.sigmoid(self.fc2(torch.sigmoid(self.fc1(X))))\n",
        "        return torch.sigmoid(self.fc2(torch.relu(self.fc(torch.sigmoid(self.fc1(X))))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de765fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de765fe",
        "outputId": "f9b1eb30-3a02-4336-a8be-6ce9357526c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.2528023421764374\n",
            "loss: 0.0015943795442581177\n",
            "loss: 0.0003419781569391489\n",
            "loss: 0.0001764642511261627\n",
            "loss: 0.00011544035805854946\n",
            "loss: 8.45494942041114e-05\n",
            "loss: 6.612023571506143e-05\n",
            "loss: 5.397929999162443e-05\n",
            "loss: 4.542222450254485e-05\n",
            "loss: 3.908346116077155e-05\n",
            "tensor([[0.9941],\n",
            "        [0.0048],\n",
            "        [0.9940],\n",
            "        [0.0066]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.05\n",
        "\n",
        "xornet = XORNet()\n",
        "\n",
        "optimizer2 = torch.optim.SGD(xornet.parameters(), lr=learning_rate, momentum=0.9) # Torch optimizer\n",
        "\n",
        "loss_fn = torch.nn.MSELoss() # Eclidean loss function\n",
        "\n",
        "for step in range(10000):\n",
        "\n",
        "  # forward pass\n",
        "  Yp = xornet(X)\n",
        "\n",
        "  # backward pass\n",
        "  optimizer2.zero_grad()  # zero out previous gradients\n",
        "  loss = loss_fn(Yp,Y)   # compute loss\n",
        "  loss.backward()        # calculate gradients\n",
        "  optimizer2.step()       # adjust parameters\n",
        "\n",
        "  # diagnostics\n",
        "  if step%1000 == 0:\n",
        "    print(\"loss:\",loss.item())\n",
        "\n",
        "print(Yp)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13a3e89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f13a3e89",
        "outputId": "752306aa-972d-4994-c86d-13f12499fddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3506, -0.9267,  0.0768,  0.9164, -1.2467, -6.2091, -2.5248,  4.7537],\n",
            "        [ 0.9317, -2.4585,  1.1709,  0.8998,  3.2911, -6.0572, -2.8577, -3.4503]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(W1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc490d02",
      "metadata": {
        "id": "dc490d02"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "807c4fd6",
      "metadata": {
        "id": "807c4fd6"
      },
      "source": [
        "# Part 2: XOR with Manual Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60d30e8",
      "metadata": {
        "id": "f60d30e8"
      },
      "outputs": [],
      "source": [
        "# This notebook implements a neural net to apprximate XOR function using PyTorch\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac7de50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eac7de50",
        "outputId": "2b2a5d46-c387-49e8-b24a-5a0343de1c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  tensor([[1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.]])\n",
            "output:  tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.tensor([[1.0,0.0,0.0,1.0],[0.0,0.0,1.0,1.0]],dtype=torch.float32)\n",
        "X = torch.transpose(X,0,1) # 4x2 matrix\n",
        "Y = torch.tensor([[1.0,0.0,1.0,0.0]],dtype=torch.float32)\n",
        "Y = torch.transpose(Y,0,1) # 4x1 vector\n",
        "print(\"input: \", X)\n",
        "print(\"output: \", Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef698e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ef698e4",
        "outputId": "c3e4fde0-b072-4697-b605-406fe19540c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.0379252433776855\n",
            "loss: 0.11125476658344269\n",
            "loss: 0.013625452294945717\n",
            "loss: 0.005919770337641239\n",
            "loss: 0.0035672527737915516\n",
            "loss: 0.0024878974072635174\n",
            "loss: 0.0018834633519873023\n",
            "loss: 0.0015023509040474892\n",
            "loss: 0.0012423645239323378\n",
            "loss: 0.0010547919664531946\n",
            "tensor([[0.9855],\n",
            "        [0.0144],\n",
            "        [0.9847],\n",
            "        [0.0161]])\n",
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "# parameters of neural net\n",
        "W1 = Variable(torch.torch.FloatTensor(2, 8).uniform_(-1, 1), requires_grad=True) # 2x8 matrix\n",
        "b1 = Variable(torch.zeros((1,8)), requires_grad=True)                            # 1x8 matrix\n",
        "W2 = Variable(torch.torch.FloatTensor(8, 1).uniform_(-1, 1), requires_grad=True) # 8x1 matrix\n",
        "b2 = Variable(torch.zeros([1]), requires_grad=True)                              # scalar\n",
        "\n",
        "learning_rate = 0.5\n",
        "\n",
        "for step in range(10000):\n",
        "\n",
        "  # forward pass\n",
        "  Z1 = torch.mm(X,W1)    # 4x8 matrix\n",
        "  Z2 = Z1 + b1           # 4x8 matrix\n",
        "  Z3 = torch.sigmoid(Z2) # 4x8 matrix\n",
        "  Z4 = torch.mm(Z3,W2)   # 4x1 vector\n",
        "  Z5 = Z4 + b2           # 4x1 vector\n",
        "  Yp = torch.sigmoid(Z5) # 4x1 vector\n",
        "\n",
        "  # backward pass\n",
        "  dYp = Yp-Y # 4x1 vector\n",
        "  dZ5 = torch.sigmoid(Z5)*(1.0-torch.sigmoid(Z5))*dYp # 4x1 vector\n",
        "  dZ4 = dZ5  # 4x1 vector\n",
        "  dZ3 = torch.mm(dZ4,torch.transpose(W2,0,1)) # 4x8 matrix\n",
        "  dZ2 = torch.sigmoid(Z2)*(1.0-torch.sigmoid(Z2))*dZ3 # 4x8 matrix\n",
        "  dZ1 = dZ2 # 4x8 matrix\n",
        "\n",
        "  dW1 = torch.mm(torch.transpose(X,0,1),dZ1)\n",
        "  db1 = torch.sum(dZ2,0,True)\n",
        "  dW2 = torch.mm(torch.transpose(Z3,0,1),dZ4)\n",
        "  db2 = torch.sum(dZ5)\n",
        "\n",
        "  # adjust parameters by gradient descent\n",
        "  W1 = W1 - learning_rate*dW1\n",
        "  b1 = b1 - learning_rate*db1\n",
        "  W2 = W2 - learning_rate*dW2\n",
        "  b2 = b2 - learning_rate*db2\n",
        "\n",
        "  if step%1000 == 0:\n",
        "    loss = torch.sum((Yp-Y)**2)\n",
        "    print(\"loss:\",loss.item())\n",
        "\n",
        "print(Yp.data)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8f1ec8",
      "metadata": {
        "id": "7d8f1ec8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3cdbef3c",
      "metadata": {
        "id": "3cdbef3c"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we explored the XOR problem using two different approaches:\n",
        "\n",
        "- **Built-in backpropagation**: We leveraged modern deep learning libraries to train a neural network quickly and efficiently. This demonstrates the power and convenience of automatic differentiation and high-level abstractions.\n",
        "\n",
        "- **Manual backpropagation**: We implemented the learning process step by step, giving us insight into how gradients are calculated and how weights are updated. This approach reinforces the mathematical foundations of neural networks.\n",
        "\n",
        "The XOR problem illustrates why non-linear activation functions and multiple layers are essential for solving tasks that cannot be handled by a single-layer perceptron. By comparing these two approaches, we see both the **practical advantages** of using libraries and the **educational value** of working through the details manually.\n",
        "\n",
        "This exercise provides a foundation for tackling more complex problems in machine learning and deep learning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}